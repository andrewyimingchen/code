from sklearn.model_selection import GridSearchCV, RandomizedSearchCV
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.linear_model import Ridge, Lasso, ElasticNet
from sklearn.svm import SVR
from sklearn.neighbors import KNeighborsRegressor
from xgboost import XGBRegressor
from lightgbm import LGBMRegressor
from sklearn.neural_network import MLPRegressor
from catboost import CatBoostRegressor

# Define parameter grids for each model
param_grids = {
    'RandomForest': {
        'n_estimators': [100, 200, 300],
        'max_depth': [None, 10, 20, 30],
        'min_samples_split': [2, 5, 10]
    },
    'GradientBoosting': {
        'n_estimators': [100, 200, 300],
        'learning_rate': [0.01, 0.05, 0.1],
        'max_depth': [3, 4, 5]
    },
    'Ridge': {
        'alpha': [0.1, 1.0, 10.0]
    },
    'Lasso': {
        'alpha': [0.01, 0.1, 1.0, 10.0]
    },
    'ElasticNet': {
        'alpha': [0.01, 0.1, 1.0],
        'l1_ratio': [0.1, 0.5, 0.9]
    },
    'SVR': {
        'C': [0.1, 1.0, 10.0],
        'kernel': ['linear', 'rbf'],
        'gamma': ['scale', 'auto']
    },
    'KNN': {
        'n_neighbors': [3, 5, 7, 9],
        'weights': ['uniform', 'distance']
    },
    'XGBoost': {
        'n_estimators': [100, 200, 300],
        'learning_rate': [0.01, 0.05, 0.1],
        'max_depth': [3, 4, 5]
    },
    'LightGBM': {
        'n_estimators': [100, 200, 300],
        'learning_rate': [0.01, 0.05, 0.1],
        'num_leaves': [31, 50, 100]
    },
    'CatBoost': {
        'iterations': [100, 200, 300],
        'learning_rate': [0.01, 0.05, 0.1]
    },
    'NeuralNetwork': {
        'hidden_layer_sizes': [(100,), (50, 50), (100, 50)],
        'max_iter': [200, 500],
        'alpha': [0.0001, 0.001]
    }
}

# Define models to be tuned
models = {
    'RandomForest': RandomForestRegressor(random_state=42),
    'GradientBoosting': GradientBoostingRegressor(random_state=42),
    'Ridge': Ridge(),
    'Lasso': Lasso(),
    'ElasticNet': ElasticNet(),
    'SVR': SVR(),
    'KNN': KNeighborsRegressor(),
    'XGBoost': XGBRegressor(random_state=42),
    'LightGBM': LGBMRegressor(random_state=42),
    'CatBoost': CatBoostRegressor(random_state=42, silent=True),
    'NeuralNetwork': MLPRegressor(random_state=42)
}

# Tuning function using GridSearchCV
def tune_model(model_name, model, param_grid, X_train, y_train):
    print(f"Tuning {model_name}...")
    grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=5, scoring='neg_mean_squared_error', n_jobs=-1, verbose=1)
    grid_search.fit(X_train, y_train)
    print(f"Best parameters for {model_name}: {grid_search.best_params_}")
    return grid_search.best_estimator_

# Example dataset (replace this with your own)
data = {
    'Age': [34, 36, 30, 29, 28],
    'Acceleration': [85, 87, 78, 77, 84],
    'Agility': [91, 89, 80, 75, 88],
    'Stamina': [72, 77, 88, 83, 76],
    'Dribbling': [95, 88, 86, 82, 90],
    'Passing': [92, 81, 93, 89, 87],
    'Overall': [93, 91, 91, 87, 90]
}

# Convert to DataFrame
df = pd.DataFrame(data)

# Features and target
X = df.drop('Overall', axis=1)
y = df['Overall']

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Tune all models and get the best models
best_models = {}
for model_name, model in models.items():
    best_model = tune_model(model_name, model, param_grids[model_name], X_train, y_train)
    best_models[model_name] = best_model
